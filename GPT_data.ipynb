{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get GPT data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching webtext.train.jsonl: 679Mit [00:58, 11.6Mit/s]                                             \n",
      "Fetching webtext.valid.jsonl: 13.6Mit [00:02, 6.61Mit/s]                                            \n",
      "Fetching webtext.test.jsonl: 13.5Mit [00:01, 13.4Mit/s]                                             \n",
      "Fetching small-117M.train.jsonl: 775Mit [02:13, 5.80Mit/s]                                          \n",
      "Fetching small-117M.valid.jsonl: 15.4Mit [00:02, 7.05Mit/s]                                         \n",
      "Fetching small-117M.test.jsonl: 15.6Mit [00:01, 9.31Mit/s]                                          \n",
      "Fetching small-117M-k40.train.jsonl:  53%|███████████▋          | 377M/711M [01:56<01:43, 3.23Mit/s]\n",
      "Fetching small-117M-k40.valid.jsonl: 14.4Mit [00:04, 3.41Mit/s]                                     \n",
      "Fetching small-117M-k40.test.jsonl: 14.4Mit [00:03, 3.81Mit/s]                                      \n",
      "Fetching medium-345M.train.jsonl:  38%|█████████▌               | 302M/791M [01:58<03:11, 2.55Mit/s]\n",
      "Fetching medium-345M.valid.jsonl: 15.8Mit [00:03, 3.96Mit/s]                                        \n",
      "Fetching medium-345M.test.jsonl: 15.8Mit [00:03, 4.08Mit/s]                                         \n",
      "Fetching medium-345M-k40.train.jsonl:  75%|███████████████▊     | 588M/784M [03:51<01:17, 2.54Mit/s]\n",
      "Fetching medium-345M-k40.valid.jsonl: 15.7Mit [00:03, 3.96Mit/s]                                    \n",
      "Fetching medium-345M-k40.test.jsonl: 15.6Mit [00:04, 3.77Mit/s]                                     \n",
      "Fetching large-762M.train.jsonl: 753Mit [03:43, 3.37Mit/s]                                          \n",
      "Fetching large-762M.valid.jsonl: 14.9Mit [00:03, 3.75Mit/s]                                         \n",
      "Fetching large-762M.test.jsonl: 15.0Mit [00:03, 3.91Mit/s]                                          \n",
      "Fetching large-762M-k40.train.jsonl: 751Mit [03:25, 3.65Mit/s]                                      \n",
      "Fetching large-762M-k40.valid.jsonl: 15.0Mit [00:03, 3.78Mit/s]                                     \n",
      "Fetching large-762M-k40.test.jsonl: 15.0Mit [00:02, 5.41Mit/s]                                      \n",
      "Fetching xl-1542M.train.jsonl: 724Mit [01:49, 6.62Mit/s]                                            \n",
      "Fetching xl-1542M.valid.jsonl: 14.4Mit [00:04, 3.24Mit/s]                                           \n",
      "Fetching xl-1542M.test.jsonl: 14.6Mit [00:04, 3.36Mit/s]                                            \n",
      "Fetching xl-1542M-k40.train.jsonl: 100%|███████████████████████▉| 747M/748M [03:49<00:00, 3.26Mit/s]\n",
      "Fetching xl-1542M-k40.valid.jsonl: 15.0Mit [00:04, 3.34Mit/s]                                       \n",
      "Fetching xl-1542M-k40.test.jsonl: 14.6Mit [00:04, 3.47Mit/s]                                        \n"
     ]
    }
   ],
   "source": [
    "subdir = r'C:\\Users\\Sten\\Documents\\EUR BAM\\Thesis\\data'\n",
    "\n",
    "for ds in [\n",
    "    'webtext',\n",
    "    'small-117M',  'small-117M-k40',\n",
    "    'medium-345M', 'medium-345M-k40',\n",
    "    'large-762M',  'large-762M-k40',\n",
    "    'xl-1542M',    'xl-1542M-k40',\n",
    "]:\n",
    "    for split in ['train', 'valid', 'test']:\n",
    "        filename = ds + \".\" + split + '.jsonl'\n",
    "        r = requests.get(\"https://openaipublic.azureedge.net/gpt-2/output-dataset/v1/\" + filename, stream=True)\n",
    "\n",
    "        with open(os.path.join(subdir, filename), 'wb') as f:\n",
    "            file_size = int(r.headers[\"content-length\"])\n",
    "            chunk_size = 1000\n",
    "            with tqdm(ncols=100, desc=\"Fetching \" + filename, total=file_size, unit_scale=True) as pbar:\n",
    "                # 1k for chunk_size, since Ethernet packet size is around 1500 bytes\n",
    "                for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(chunk_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
